<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第14章：神经逆向渲染</title>
    <link rel="stylesheet" href="./assets/style.css">
    <link rel="stylesheet" href="./assets/highlight.css">
    <script src="./assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <ul class="nav-list"><li class=""><a href="./index.html">统一计算机图形学：从几何光学到波动物理</a></li><li class=""><a href="./chapter1.html">第1章：几何光学与渲染基础</a></li><li class=""><a href="./chapter2.html">第2章：光场与近场光学</a></li><li class=""><a href="./chapter3.html">第三章：基于点的渲染与统一体渲染方程</a></li><li class=""><a href="./chapter4.html">第4章：基于图像的渲染</a></li><li class=""><a href="./chapter5.html">第5章：基于物理的渲染</a></li><li class=""><a href="./chapter6.html">第6章：神经辐射场基础（NeRF）</a></li><li class=""><a href="./chapter7.html">第7章：动态神经辐射场</a></li><li class=""><a href="./chapter8.html">第8章：4D神经表示与频域方法</a></li><li class=""><a href="./chapter9.html">第9章：显式神经表示</a></li><li class=""><a href="./chapter10.html">第10章：3D高斯溅射</a></li><li class=""><a href="./chapter11.html">第11章：逆向渲染的数学基础</a></li><li class=""><a href="./chapter12.html">第12章：可微渲染 (Differentiable Rendering)</a></li><li class=""><a href="./chapter13.html">第13章：材质与几何重建</a></li><li class="active"><a href="./chapter14.html">第14章：神经逆向渲染</a></li><li class=""><a href="./chapter15.html">第15章：标量波动光学基础</a></li><li class=""><a href="./chapter16.html">第16章：相干理论</a></li><li class=""><a href="./chapter17.html">第17章：波前整形与自适应光学</a></li><li class=""><a href="./chapter18.html">第18章：统计光学与散斑</a></li><li class=""><a href="./chapter19.html">第19章：衍射理论与计算方法</a></li><li class=""><a href="./chapter20.html">第20章：非线性光学与显微成像</a></li><li class=""><a href="./chapter21.html">第21章：半经典光-物质相互作用</a></li><li class=""><a href="./chapter22.html">第22章：偏振光学基础</a></li><li class=""><a href="./chapter23.html">第23章：偏振渲染</a></li><li class=""><a href="./chapter24.html">第24章：全息显示与计算全息</a></li><li class=""><a href="./chapter25.html">第25章：超材料与变换光学</a></li><li class=""><a href="./chapter26.html">第26章：拓扑光子学</a></li><li class=""><a href="./chapter27.html">第27章：量子光学基础</a></li><li class=""><a href="./chapter28.html">第28章：量子成像与计算</a></li><li class=""><a href="./CLAUDE.html">Untitled</a></li></ul>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="14">第14章：神经逆向渲染</h1>
<p>神经逆向渲染将深度学习与物理渲染原理相结合，从观测图像中推断场景的三维结构、材质属性和光照条件。本章探讨如何利用神经表示解决渲染方程的逆问题，包括分解表示、生成先验的引入、少样本学习以及实时系统的设计。我们将看到，通过巧妙地参数化和约束，可以将不适定的逆向问题转化为可优化的目标函数。这一领域结合了计算机视觉、机器学习和计算机图形学的前沿技术，在虚拟现实、增强现实、数字孪生等应用中具有重要价值。</p>
<h2 id="_1">学习目标</h2>
<p>完成本章后，您将能够：</p>
<ol>
<li>设计神经场的逆优化框架，理解梯度流与收敛性</li>
<li>构建分解表示，分离几何、材质和光照的相互作用</li>
<li>利用生成模型作为先验知识，正则化逆向重建</li>
<li>实现少样本重建算法，从有限观测中恢复完整场景</li>
<li>分析实时逆向渲染系统的设计权衡</li>
</ol>
<h2 id="141">14.1 神经场的逆优化</h2>
<h3 id="1411">14.1.1 从正向渲染到逆向推断</h3>
<p>在第6-9章中，我们学习了神经辐射场的正向渲染：给定场景表示 $\Theta$，生成图像 $I = \mathcal{R}(\Theta)$。逆向渲染则是求解逆问题：</p>
<p>$$\Theta^* = \arg\min_\Theta \mathcal{L}(\mathcal{R}(\Theta), I_{\text{obs}}) + \lambda \mathcal{R}_{\text{reg}}(\Theta)$$
其中 $I_{\text{obs}}$ 是观测图像，$\mathcal{L}$ 是重建损失，$\mathcal{R}_{\text{reg}}$ 是正则化项。</p>
<p>这个优化问题的核心挑战在于：</p>
<ol>
<li><strong>非凸性</strong>：目标函数通常有多个局部极小值</li>
<li><strong>高维性</strong>：参数空间维度可达百万级</li>
<li><strong>不适定性</strong>：多个参数配置可能产生相似的渲染结果</li>
<li><strong>计算成本</strong>：每次梯度计算需要完整的渲染过程</li>
</ol>
<p>对于神经场表示，参数 $\Theta$ 包含：</p>
<ul>
<li>网络权重 $\mathbf{W} = {W_i, b_i}_{i=1}^L$</li>
<li>位置编码参数 $\gamma$</li>
<li>特征网格或其他显式表示</li>
</ul>
<p><strong>信息论视角</strong>：逆向渲染可以视为信息恢复问题。观测图像 $I_{\text{obs}}$ 包含的信息量为：
$$I(I_{\text{obs}}; \Theta) = H(\Theta) - H(\Theta|I_{\text{obs}})$$
其中 $H(\cdot)$ 是熵函数。逆向渲染的目标是最大化这个互信息。</p>
<p>逆向渲染的数学框架可以从变分推断角度理解。给定观测 $I_{\text{obs}}$，我们希望推断后验分布：
$$p(\Theta|I_{\text{obs}}) = \frac{p(I_{\text{obs}}|\Theta)p(\Theta)}{p(I_{\text{obs}})}$$
点估计 $\Theta^<em>$ 对应最大后验（MAP）估计：
$$\Theta^</em> = \arg\max_\Theta \log p(I_{\text{obs}}|\Theta) + \log p(\Theta)$$
其中 $-\log p(I_{\text{obs}}|\Theta)$ 对应重建损失，$-\log p(\Theta)$ 对应正则化项。</p>
<p><strong>Hadamard 不适定性分析</strong>：逆向渲染问题通常违反 Hadamard 适定性条件：</p>
<ol>
<li><strong>存在性</strong>：解可能不存在（噪声数据）</li>
<li><strong>唯一性</strong>：多个场景可能产生相同图像</li>
<li><strong>稳定性</strong>：小扰动可能导致大变化</li>
</ol>
<p>形式化地，条件数：
$$\kappa = \frac{|\delta \Theta|}{|\Theta|} / \frac{|\delta I|}{|I|}$$
对于逆向渲染，$\kappa \gg 1$，表明问题的不适定性。</p>
<h3 id="1412">14.1.2 神经场参数化的梯度计算</h3>
<p>考虑标准的体积渲染方程：
$$C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt$$
其中透射率 $T(t) = \exp\left(-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds\right)$。</p>
<p><strong>伴随状态方法</strong>：为了高效计算梯度，引入伴随变量 $\lambda(t)$：
$$\lambda(t) = \frac{\partial \mathcal{L}}{\partial T(t)}$$
伴随方程为：
$$\frac{d\lambda}{dt} = -\lambda(t) \sigma(\mathbf{r}(t)) - \frac{\partial \mathcal{L}}{\partial \mathbf{c}(\mathbf{r}(t))}$$
边界条件：$\lambda(t_f) = 0$。</p>
<p>对网络参数 $\mathbf{W}$ 的梯度通过链式法则计算：
$$\frac{\partial \mathcal{L}}{\partial \mathbf{W}} = \sum_{\mathbf{r}} \frac{\partial \mathcal{L}}{\partial C(\mathbf{r})} \frac{\partial C(\mathbf{r})}{\partial \mathbf{W}}$$
关键在于计算 $\frac{\partial C(\mathbf{r})}{\partial \mathbf{W}}$：
$$\frac{\partial C(\mathbf{r})}{\partial \mathbf{W}} = \int_{t_n}^{t_f} \left[ \frac{\partial T(t)}{\partial \mathbf{W}} \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) + T(t) \frac{\partial}{\partial \mathbf{W}}[\sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d})] \right] dt$$
其中透射率的梯度涉及路径积分：
$$\frac{\partial T(t)}{\partial \mathbf{W}} = -T(t) \int_{t_n}^t \frac{\partial \sigma(\mathbf{r}(s))}{\partial \mathbf{W}} ds$$
<strong>隐式微分技术</strong>：对于隐式定义的表面 $F(\mathbf{x}, \Theta) = 0$，使用隐函数定理：
$$\frac{\partial \mathbf{x}}{\partial \Theta} = -\left(\frac{\partial F}{\partial \mathbf{x}}\right)^{-1} \frac{\partial F}{\partial \Theta}$$
<strong>数值实现的关键考虑</strong>：</p>
<ol>
<li>
<p><strong>离散化</strong>：使用分层采样将积分转化为求和
$$C(\mathbf{r}) \approx \sum_{i=1}^N T_i \alpha_i \mathbf{c}_i$$
其中 $\alpha_i = 1 - \exp(-\sigma_i \delta_i)$，$T_i = \prod_{j=1}^{i-1}(1-\alpha_j)$</p>
</li>
<li>
<p><strong>梯度累积</strong>：反向传播时需要正确处理累积透射率
$$\frac{\partial T_i}{\partial \alpha_j} = \begin{cases}
   -\prod_{k=1}^{j-1}(1-\alpha_k) \prod_{k=j+1}^{i-1}(1-\alpha_k) &amp; j &lt; i \
   0 &amp; j \geq i
   \end{cases}$$</p>
</li>
<li>
<p><strong>数值稳定性</strong>：使用对数空间计算防止数值下溢
$$\log T_i = \sum_{j=1}^{i-1} \log(1-\alpha_j)$$</p>
</li>
<li>
<p><strong>重要性采样</strong>：根据透射率分布自适应调整采样密度
$$p(t) \propto T(t)\sigma(t)$$</p>
</li>
<li>
<p><strong>梯度估计的方差减少</strong>：
   - <strong>控制变量</strong>：$\nabla_{\text{CV}} = \nabla - c(\nabla_{\text{baseline}} - \mathbb{E}[\nabla_{\text{baseline}}])$
   - <strong>Rao-Blackwellization</strong>：利用条件期望减少方差
   - <strong>多重重要性采样</strong>：结合多个采样策略</p>
</li>
</ol>
<p><strong>二阶优化方法</strong>：</p>
<p>Hessian 矩阵的近似计算：
$$\mathbf{H} \approx \mathbf{J}^T\mathbf{J} + \lambda\mathbf{I}$$
其中 $\mathbf{J}$ 是 Jacobian 矩阵。使用 Gauss-Newton 或 Levenberg-Marquardt 方法可以提高收敛速度：
$$\mathbf{W}_{k+1} = \mathbf{W}_k - (\mathbf{H} + \mu\mathbf{I})^{-1}\nabla_{\mathbf{W}}\mathcal{L}$$
<strong>自然梯度方法</strong>：考虑参数空间的黎曼几何：
$$\mathbf{W}_{k+1} = \mathbf{W}_k - \eta \mathbf{F}^{-1} \nabla_{\mathbf{W}}\mathcal{L}$$
其中 $\mathbf{F}$ 是 Fisher 信息矩阵：
$$\mathbf{F} = \mathbb{E}\left[\left(\frac{\partial \log p(I|\Theta)}{\partial \Theta}\right)\left(\frac{\partial \log p(I|\Theta)}{\partial \Theta}\right)^T\right]$$</p>
<h3 id="1413">14.1.3 多视图一致性约束</h3>
<p>给定 $N$ 个视图 ${I_i}_{i=1}^N$ 及其相机参数 ${\mathbf{P}_i}_{i=1}^N$，多视图重建损失为：
$$\mathcal{L}_{\text{multi}} = \sum_{i=1}^N \sum_{\mathbf{p} \in I_i} \rho\left( |\mathcal{R}(\Theta, \mathbf{P}_i, \mathbf{p}) - I_i(\mathbf{p})|_2 \right)$$
其中 $\rho$ 是鲁棒损失函数（如 Huber loss）。</p>
<p><strong>Huber 损失的选择理由</strong>：
$$\rho(x) = \begin{cases}
\frac{1}{2}x^2 &amp; |x| \leq \delta \
\delta(|x| - \frac{1}{2}\delta) &amp; |x| &gt; \delta
\end{cases}$$
这提供了 L2 损失的平滑性和 L1 损失的鲁棒性，对离群点不敏感。</p>
<p><strong>几何一致性</strong>通过深度图约束实现：
$$\mathcal{L}_{\text{depth}} = \sum_{i,j} \sum_{\mathbf{p}} w_{ij}(\mathbf{p}) |D_i(\mathbf{p}) - \Pi_{ij}(D_j(\Pi_{ji}(\mathbf{p})))|$$
其中 $\Pi_{ij}$ 是从视图 $j$ 到视图 $i$ 的投影变换，$w_{ij}$ 是可见性权重。</p>
<p><strong>深度重投影的数学细节</strong>：给定深度 $d_j$ 和像素 $\mathbf{p}_j$，3D 点为：
$$\mathbf{X} = d_j \mathbf{K}_j^{-1}[\mathbf{p}_j; 1]$$
投影到视图 $i$：
$$[\mathbf{p}_i; 1] = \mathbf{K}_i \mathbf{R}_{ij}(\mathbf{X} - \mathbf{t}_{ij})/z_i$$
<strong>光度一致性约束</strong>：</p>
<p>考虑表面点 $\mathbf{X}$ 在不同视图下的投影：
$$\mathbf{p}_i = \mathbf{P}_i\mathbf{X}, \quad \mathbf{p}_j = \mathbf{P}_j\mathbf{X}$$
光度一致性要求：
$$\mathcal{L}_{\text{photo}} = \sum_{\mathbf{X}} \sum_{i,j} v_{ij}(\mathbf{X}) |I_i(\mathbf{p}_i) - I_j(\mathbf{p}_j)|$$
其中可见性函数 $v_{ij}(\mathbf{X})$ 检查 $\mathbf{X}$ 在两个视图中是否都可见。</p>
<p><strong>遮挡感知的可见性计算</strong>：
$$v_{ij}(\mathbf{X}) = \exp\left(-\alpha \max(0, D_i(\mathbf{p}_i) - d_i(\mathbf{X}))\right)$$
其中 $d_i(\mathbf{X})$ 是 $\mathbf{X}$ 到相机 $i$ 的距离。</p>
<p><strong>极线约束</strong>：</p>
<p>对于对应点 $\mathbf{p}_i$、$\mathbf{p}_j$，必须满足：
$$\mathbf{p}_j^T \mathbf{F}_{ij} \mathbf{p}_i = 0$$
其中 $\mathbf{F}_{ij}$ 是基础矩阵。这可以作为软约束加入：
$$\mathcal{L}_{\text{epipolar}} = \sum_{(\mathbf{p}_i, \mathbf{p}_j)} \frac{(\mathbf{p}_j^T \mathbf{F}_{ij} \mathbf{p}_i)^2}{|\mathbf{F}_{ij}^T\mathbf{p}_j|^2_{[1:2]} + |\mathbf{F}_{ij}\mathbf{p}_i|^2_{[1:2]}}$$
分母项（Sampson 距离）提供了几何上有意义的归一化。</p>
<p><strong>多尺度一致性</strong>：
$$\mathcal{L}_{\text{multiscale}} = \sum_{s=0}^S w_s |\mathcal{R}_s(\Theta) - \text{Downsample}^s(I_{\text{obs}})|$$
其中 $\mathcal{R}_s$ 是尺度 $s$ 的渲染器。</p>
<p><strong>时间一致性</strong>（对于视频序列）：
$$\mathcal{L}_{\text{temporal}} = \sum_t |\Theta(t) - \Theta(t-1)|^2_{\text{smooth}} + \sum_t |\mathcal{F}(\Theta(t), \Theta(t-1))|^2$$
其中 $\mathcal{F}$ 是光流或场景流约束。</p>
<p><strong>场景流的具体形式</strong>：
$$\mathcal{F}(\Theta_t, \Theta_{t-1}) = \int_{\mathcal{V}} |\mathbf{v}(\mathbf{x}, t) - \frac{\mathbf{x}_t - \mathbf{x}_{t-1}}{\Delta t}|^2 d\mathbf{x}$$
其中 $\mathbf{v}$ 是预测的速度场。</p>
<p><strong>循环一致性约束</strong>：
$$\mathcal{L}_{\text{cycle}} = \sum_{i,j,k} |\mathbf{p}_i - \Pi_{ik}^{-1}(\Pi_{kj}^{-1}(\Pi_{ji}(\mathbf{p}_i)))|$$
<strong>联合优化框架</strong>：
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{multi}} + \lambda_d \mathcal{L}_{\text{depth}} + \lambda_p \mathcal{L}_{\text{photo}} + \lambda_e \mathcal{L}_{\text{epipolar}} + \lambda_t \mathcal{L}_{\text{temporal}}$$
权重 ${\lambda_d, \lambda_p, \lambda_e, \lambda_t}$ 需要仔细调节以平衡不同约束的贡献。</p>
<p><strong>自适应权重策略</strong>：使用不确定性加权：
$$\mathcal{L}_{\text{total}} = \sum_i \frac{1}{2\sigma_i^2}\mathcal{L}_i + \log \sigma_i$$
其中 $\sigma_i$ 是可学习的不确定性参数。</p>
<h3 id="1414">14.1.4 正则化策略与收敛性分析</h3>
<p><strong>密度正则化</strong>：促进紧凑的几何表示
$$\mathcal{R}_{\text{density}} = \int_{\mathcal{V}} \sigma(\mathbf{x}) \log \sigma(\mathbf{x}) d\mathbf{x}$$
这是信息熵的负值，鼓励稀疏的密度分布。</p>
<p><strong>信息论解释</strong>：最小化熵等价于最大化密度分布的确定性，防止"雾状"解。</p>
<p><strong>平滑性正则化</strong>：
$$\mathcal{R}_{\text{smooth}} = \int_{\mathcal{V}} |\nabla \sigma(\mathbf{x})|^2 + |\nabla \mathbf{c}(\mathbf{x})|^2 d\mathbf{x}$$
<strong>Sobolev 空间视角</strong>：这对应于 $H^1$ 范数，控制函数的 Sobolev 正则性。</p>
<p><strong>分布正则化</strong>：</p>
<ol>
<li>
<p><strong>Lipschitz 正则化</strong>：限制函数的变化率
$$\mathcal{R}_{\text{Lip}} = \mathbb{E}_{\mathbf{x}_1, \mathbf{x}_2}\left[\max\left(0, \frac{|f(\mathbf{x}_1) - f(\mathbf{x}_2)|}{|\mathbf{x}_1 - \mathbf{x}_2|} - L\right)^2\right]$$</p>
</li>
<li>
<p><strong>TV 正则化</strong>：保持边缘的同时促进平滑
$$\mathcal{R}_{\text{TV}} = \int_{\mathcal{V}} |\nabla \sigma(\mathbf{x})|_1 d\mathbf{x}$$
<strong>各向异性 TV</strong>：
$$\mathcal{R}_{\text{ATV}} = \int_{\mathcal{V}} \sum_{i=1}^3 |\partial_i \sigma(\mathbf{x})| d\mathbf{x}$$</p>
</li>
<li>
<p><strong>频谱正则化</strong>：在频域控制复杂度
$$\mathcal{R}_{\text{freq}} = \int_{\boldsymbol{\omega}} |\boldsymbol{\omega}|^{2s} |\hat{f}(\boldsymbol{\omega})|^2 d\boldsymbol{\omega}$$
其中 $s &gt; 0$ 控制平滑度（$s=1$ 对应 $H^1$ 正则化）。</p>
</li>
</ol>
<p><strong>几何正则化</strong>：</p>
<ol>
<li>
<p><strong>曲率正则化</strong>：
$$\mathcal{R}_{\text{curv}} = \int_{\mathcal{S}} (\kappa_1^2 + \kappa_2^2) dS$$
其中 $\kappa_1, \kappa_2$ 是主曲率。</p>
</li>
<li>
<p><strong>最小表面正则化</strong>：
$$\mathcal{R}_{\text{area}} = \int_{\mathcal{S}} dS$$
<strong>收敛性分析</strong>：考虑梯度下降 $\Theta_{k+1} = \Theta_k - \eta_k \nabla_\Theta \mathcal{L}$</p>
</li>
</ol>
<p>在适当的条件下（Lipschitz 连续梯度，凸性），收敛速率为：
$$\mathcal{L}(\Theta_k) - \mathcal{L}(\Theta^<em>) \leq \frac{|\Theta_0 - \Theta^</em>|^2}{2\sum_{i=0}^{k-1} \eta_i}$$
对于非凸神经场，通常只能保证收敛到局部极小值。</p>
<p><strong>非凸优化的理论保证</strong>：</p>
<ol>
<li>
<p><strong>梯度主导条件</strong>（Polyak-Łojasiewicz）：如果满足
$$|\nabla \mathcal{L}(\Theta)|^2 \geq 2\mu (\mathcal{L}(\Theta) - \mathcal{L}(\Theta^<em>))$$
则梯度下降线性收敛：
$$\mathcal{L}(\Theta_k) - \mathcal{L}(\Theta^</em>) \leq (1-2\mu\eta)^k(\mathcal{L}(\Theta_0) - \mathcal{L}(\Theta^*))$$</p>
</li>
<li>
<p><strong>景观分析</strong>：对于过参数化网络，局部极小值往往接近全局最优
$$\mathbb{P}[\mathcal{L}(\Theta_{\text{local}}) - \mathcal{L}(\Theta^*) &gt; \epsilon] \leq \exp(-cm\epsilon^2)$$
其中 $m$ 是网络宽度，$c$ 是常数。</p>
</li>
<li>
<p><strong>逃逸鞍点</strong>：使用随机梯度或二阶方法
$$\Theta_{k+1} = \Theta_k - \eta\nabla\mathcal{L} + \sqrt{2\eta\beta^{-1}}\xi_k$$
其中 $\xi_k \sim \mathcal{N}(0, \mathbf{I})$ 是噪声项。</p>
</li>
</ol>
<p><strong>逃逸时间分析</strong>：
$$\mathbb{E}[T_{\text{escape}}] = O\left(\frac{\log(1/\epsilon)}{\lambda_{\min}(\mathbf{H})}\right)$$
其中 $\lambda_{\min}(\mathbf{H})$ 是 Hessian 的最小负特征值。</p>
<p><strong>自适应学习率策略</strong>：</p>
<ol>
<li>
<p><strong>余弦退火</strong>：
$$\eta_k = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\pi k/K))$$</p>
</li>
<li>
<p><strong>多项式衰减</strong>：
$$\eta_k = \eta_0 (1 - k/K)^p$$</p>
</li>
<li>
<p><strong>重启机制</strong>：周期性重置学习率以逃离局部极小值
$$\eta_k = \eta_0 \cdot 2^{-\lfloor k/T \rfloor}$$
其中 $T$ 是重启周期。</p>
</li>
<li>
<p><strong>自适应方法的统一视角</strong>：
$$\Theta_{k+1} = \Theta_k - \eta \mathbf{G}_k^{-1/2} \nabla\mathcal{L}$$</p>
</li>
</ol>
<ul>
<li>Adam: $\mathbf{G}_k = \text{diag}(\mathbf{v}_k) + \epsilon\mathbf{I}$</li>
<li>AdaGrad: $\mathbf{G}_k = \sum_{i=1}^k \nabla_i \nabla_i^T$</li>
<li>Natural Gradient: $\mathbf{G}_k = \mathbf{F}_k$ (Fisher 信息矩阵)</li>
</ul>
<p><strong>收敛性诊断</strong>：</p>
<ol>
<li><strong>梯度范数监控</strong>：$|\nabla\mathcal{L}| &lt; \epsilon_g$</li>
<li><strong>损失平台检测</strong>：$|\mathcal{L}_k - \mathcal{L}_{k-w}|/w &lt; \epsilon_l$</li>
<li><strong>参数变化</strong>：$|\Theta_k - \Theta_{k-1}| &lt; \epsilon_\theta$</li>
</ol>
<h2 id="142">14.2 分解表示：几何、材质、光照</h2>
<h3 id="1421">14.2.1 内在图像分解理论</h3>
<p>场景的外观可以分解为内在成分的乘积：
$$I(\mathbf{x}) = A(\mathbf{x}) \cdot S(\mathbf{x}) \cdot \text{vis}(\mathbf{x})$$
其中：</p>
<ul>
<li>$A(\mathbf{x})$：反照率（材质颜色）</li>
<li>$S(\mathbf{x})$：着色（光照效果）</li>
<li>$\text{vis}(\mathbf{x})$：可见性/阴影</li>
</ul>
<p>在神经渲染框架下，我们将这种分解扩展到体积表示。</p>
<p><strong>理论基础</strong>：</p>
<p>内在图像分解问题可以形式化为求解以下方程组：
$$\begin{cases}
I(\mathbf{x}) = A(\mathbf{x}) \odot S(\mathbf{x}) \
\nabla \cdot (\nabla S) = \text{sparse} \quad \text{(着色平滑)} \
A(\mathbf{x}) \in [0, 1]^3 \quad \text{(反照率范围)}
\end{cases}$$
这是一个欠定系统，因为我们有 3 个未知通道（RGB）但只有 3 个约束。</p>
<p><strong>Retinex 理论</strong>：</p>
<p>根据 Land 的 Retinex 理论，人类视觉系统假设：</p>
<ol>
<li>照明在空间上缓慢变化</li>
<li>反照率在物体边界处不连续</li>
</ol>
<p>数学上，在对数域：
$$\log I = \log A + \log S$$
使用高通滤波器分离：
$$\log A \approx \text{HPF}(\log I)$$
$$\log S \approx \text{LPF}(\log I)$$
<strong>体积渲染中的分解</strong>：</p>
<p>对于体积表示，分解变得更加复杂：
$$C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(t) \left[ A(\mathbf{r}(t)) \odot \int_{\mathbb{S}^2} f_r L(\mathbf{r}(t), \boldsymbol{\omega}) (\boldsymbol{\omega} \cdot \mathbf{n})^+ d\boldsymbol{\omega} \right] dt$$
其中：</p>
<ul>
<li>$A(\mathbf{x})$：体积反照率</li>
<li>$L(\mathbf{x}, \boldsymbol{\omega})$：入射光照</li>
<li>$f_r$：体积 BRDF</li>
<li>$\mathbf{n}$：由密度梯度计算的法线</li>
</ul>
<h3 id="1422">14.2.2 神经场中的分解架构</h3>
<p><strong>分解的神经辐射场</strong>表示为多个专门网络：</p>
<ol>
<li>
<p><strong>几何网络</strong> $f_\sigma: \mathbb{R}^3 \rightarrow \mathbb{R}^+$
$$\sigma(\mathbf{x}) = f_\sigma(\gamma_{\text{geo}}(\mathbf{x}); \Theta_\sigma)$$</p>
</li>
<li>
<p><strong>材质网络</strong> $f_A: \mathbb{R}^3 \rightarrow \mathbb{R}^3$
$$\mathbf{a}(\mathbf{x}) = f_A(\gamma_{\text{mat}}(\mathbf{x}); \Theta_A)$$</p>
</li>
<li>
<p><strong>光照网络</strong> $f_L: \mathbb{R}^3 \times \mathbb{S}^2 \rightarrow \mathbb{R}^3$
$$\mathbf{l}(\mathbf{x}, \boldsymbol{\omega}) = f_L(\mathbf{x}, \boldsymbol{\omega}; \Theta_L)$$
最终颜色通过渲染方程计算：
$$\mathbf{c}(\mathbf{x}, \boldsymbol{\omega}_o) = \mathbf{a}(\mathbf{x}) \odot \int_{\mathbb{S}^2} f_r(\boldsymbol{\omega}_i, \boldsymbol{\omega}_o, \mathbf{n}) \mathbf{l}(\mathbf{x}, \boldsymbol{\omega}_i) (\boldsymbol{\omega}_i \cdot \mathbf{n})^+ d\boldsymbol{\omega}_i$$
其中 $f_r$ 是 BRDF，$\mathbf{n}$ 是表面法线（从密度梯度计算）。</p>
</li>
</ol>
<p><strong>高级架构设计</strong>：</p>
<ol>
<li>
<p><strong>分支网络架构</strong>：
   <code>输入 → 共享编码器 → [几何分支, 材质分支, 光照分支]</code>
   共享编码器提取通用特征：
$$\mathbf{h} = f_{\text{enc}}(\mathbf{x}; \Theta_{\text{enc}})$$
各分支使用专门解码器：
$$\sigma = f_{\sigma}(\mathbf{h}; \Theta_{\sigma})$$
   $$\mathbf{a} = f_{A}(\mathbf{h}; \Theta_{A})$$
   $$\mathbf{l} = f_{L}(\mathbf{h}, \boldsymbol{\omega}; \Theta_{L})$$</p>
</li>
<li>
<p><strong>注意力机制</strong>：
   使用交叉注意力促进分支间信息交流：
$$\mathbf{h}_{\text{geo}} = \text{Attention}(Q_{\text{geo}}, K_{\text{mat}}, V_{\text{mat}})$$</p>
</li>
<li>
<p><strong>级联细化</strong>：
   逐步增加复杂度：</p>
</li>
</ol>
<ul>
<li>第1阶段：只优化几何</li>
<li>第2阶段：固定几何，优化材质+光照</li>
<li>第3阶段：联合精调所有组件</li>
</ul>
<p><strong>BRDF 参数化</strong>：</p>
<p>使用神经网络学习 BRDF：
$$f_r(\boldsymbol{\omega}_i, \boldsymbol{\omega}_o, \mathbf{n}) = f_{\text{BRDF}}(\boldsymbol{\omega}_i \cdot \mathbf{n}, \boldsymbol{\omega}_o \cdot \mathbf{n}, \boldsymbol{\omega}_i \cdot \boldsymbol{\omega}_o; \Theta_{\text{BRDF}})$$
或使用解析模型（如 Disney BRDF）：
$$f_r = \frac{\mathbf{a}}{\pi}(1-F)(1-\text{metallic}) + F\cdot G \cdot D$$
其中 $F$ 是 Fresnel 项，$G$ 是几何遮蔽，$D$ 是微面分布。</p>
<h3 id="1423">14.2.3 物理约束与解耦损失</h3>
<p><strong>反照率一致性</strong>：相同材质在不同光照下应保持恒定
$$\mathcal{L}_{\text{albedo}} = \sum_{i,j} \sum_{\mathbf{x} \in \mathcal{M}} |\mathbf{a}_i(\mathbf{x}) - \mathbf{a}_j(\mathbf{x})|^2$$
其中 $\mathcal{M}$ 是跨视图对应的表面点集。</p>
<p><strong>光照平滑性</strong>：自然光照通常是低频的
$$\mathcal{L}_{\text{light}} = \int_{\mathbb{S}^2} |\nabla_{\boldsymbol{\omega}} \mathbf{l}(\mathbf{x}, \boldsymbol{\omega})|^2 d\boldsymbol{\omega}$$
<strong>白平衡约束</strong>：场景平均反照率接近灰色
$$\mathcal{L}_{\text{white}} = \left| \frac{1}{|\mathcal{V}|} \int_{\mathcal{V}} \mathbf{a}(\mathbf{x}) d\mathbf{x} - \mathbf{a}_{\text{ref}} \right|^2$$
<strong>高级物理约束</strong>：</p>
<ol>
<li>
<p><strong>能量守恒</strong>：
$$\int_{\mathbb{S}^2} f_r(\boldsymbol{\omega}_i, \boldsymbol{\omega}_o, \mathbf{n}) (\boldsymbol{\omega}_i \cdot \mathbf{n})^+ d\boldsymbol{\omega}_i \leq 1$$
损失函数：
$$\mathcal{L}_{\text{energy}} = \max\left(0, \int_{\mathbb{S}^2} f_r (\boldsymbol{\omega}_i \cdot \mathbf{n})^+ d\boldsymbol{\omega}_i - 1\right)^2$$</p>
</li>
<li>
<p><strong>互易性</strong>：
$$f_r(\boldsymbol{\omega}_i, \boldsymbol{\omega}_o, \mathbf{n}) = f_r(\boldsymbol{\omega}_o, \boldsymbol{\omega}_i, \mathbf{n})$$
损失函数：
$$\mathcal{L}_{\text{reciprocity}} = |f_r(\boldsymbol{\omega}_i, \boldsymbol{\omega}_o, \mathbf{n}) - f_r(\boldsymbol{\omega}_o, \boldsymbol{\omega}_i, \mathbf{n})|^2$$</p>
</li>
<li>
<p><strong>阴影一致性</strong>：
   软阴影应与几何遮挡一致：
$$\mathcal{L}_{\text{shadow}} = \sum_{\mathbf{x}} |V(\mathbf{x}) - V_{\text{geo}}(\mathbf{x})|^2$$
其中 $V(\mathbf{x})$ 是学习的可见性，$V_{\text{geo}}$ 是从几何计算的可见性。</p>
</li>
<li>
<p><strong>材质稀疏性</strong>：
   鼓励材质分布的稀疏性：
$$\mathcal{L}_{\text{sparse}} = \sum_{\mathbf{x}} |\nabla \mathbf{a}(\mathbf{x})|_0$$
使用 $L_1$ 范数作为 $L_0$ 的凸松弛：
$$\mathcal{L}_{\text{sparse}} \approx \sum_{\mathbf{x}} |\nabla \mathbf{a}(\mathbf{x})|_1$$
<strong>解耦损失设计</strong>：</p>
</li>
</ol>
<p>为了促进不同组件的解耦，设计以下损失：</p>
<ol>
<li>
<p><strong>正交性损失</strong>：
$$\mathcal{L}_{\text{ortho}} = \sum_{i \neq j} |\langle \mathbf{f}_i, \mathbf{f}_j \rangle|$$
其中 $\mathbf{f}_i$ 是第 $i$ 个组件的特征表示。</p>
</li>
<li>
<p><strong>信息瓶颈</strong>：
   限制共享信息：
$$\mathcal{L}_{\text{MI}} = I(\mathbf{h}_{\text{geo}}; \mathbf{h}_{\text{mat}}) + I(\mathbf{h}_{\text{mat}}; \mathbf{h}_{\text{light}})$$
其中 $I(\cdot;\cdot)$ 是互信息。</p>
</li>
<li>
<p><strong>对抗性训练</strong>：
   使用判别器确保分解的质量：
$$\mathcal{L}_{\text{adv}} = \mathbb{E}[\log D(\mathbf{a}_{\text{real}})] + \mathbb{E}[\log(1 - D(\mathbf{a}_{\text{pred}}))]$$</p>
</li>
</ol>
<h3 id="1424">14.2.4 歧义性与规范化</h3>
<p>分解问题存在固有歧义性：</p>
<ul>
<li><strong>尺度歧义</strong>：$(k\mathbf{a}, \mathbf{l}/k)$ 产生相同外观</li>
<li><strong>颜色偏移</strong>：全局颜色变换的不确定性</li>
</ul>
<p><strong>规范化策略</strong>：</p>
<ol>
<li><strong>固定尺度</strong>：约束 $|\mathbf{a}|_\infty \leq 1$</li>
<li><strong>参考锚点</strong>：指定某些点的已知反照率</li>
<li><strong>统计先验</strong>：利用自然图像统计
$$\mathcal{L}_{\text{prior}} = D_{\text{KL}}(p(\mathbf{a}) | p_{\text{natural}}(\mathbf{a}))$$
<strong>数学分析</strong>：</li>
</ol>
<p>分解问题的解空间可以表示为：
$$\mathcal{S} = {(\mathbf{a}, \mathbf{l}) : \mathbf{a} \odot \mathbf{l} = \mathbf{c}_{\text{obs}}}$$
这是一个流形，其维度为：
$$\dim(\mathcal{S}) = \dim(\mathbf{a}) + \dim(\mathbf{l}) - \dim(\mathbf{c})$$
<strong>解决歧义性的方法</strong>：</p>
<ol>
<li>
<p><strong>引入物理先验</strong>：
   - 材质库：$\mathbf{a} \in {\mathbf{a}_1, \mathbf{a}_2, ..., \mathbf{a}_K}$
   - 光照模板：$\mathbf{l} = \sum_{i=1}^M \alpha_i \mathbf{l}_i^{\text{basis}}$</p>
</li>
<li>
<p><strong>多模态约束</strong>：
   - 深度信息：$\mathcal{L}_{\text{depth}} = |D_{\text{pred}} - D_{\text{sensor}}|^2$
   - 法线信息：$\mathcal{L}_{\text{normal}} = 1 - \mathbf{n}_{\text{pred}} \cdot \mathbf{n}_{\text{sensor}}$</p>
</li>
<li>
<p><strong>时空连贯性</strong>：
   - 时间连贯：$\mathcal{L}_{\text{temporal}} = |\mathbf{a}_t - \mathbf{a}_{t-1}|^2$
   - 空间连贯：$\mathcal{L}_{\text{spatial}} = \sum_{(i,j) \in \mathcal{N}} w_{ij}|\mathbf{a}_i - \mathbf{a}_j|^2$</p>
</li>
<li>
<p><strong>主动学习</strong>：
   选择最能减少歧义性的视角：
$$\mathbf{v}_{\text{next}} = \arg\max_{\mathbf{v}} H(\mathbf{a}|\mathbf{c}_{1:n}) - H(\mathbf{a}|\mathbf{c}_{1:n}, \mathbf{c}_\mathbf{v})$$
<strong>理论保证</strong>：</p>
</li>
</ol>
<p>在以下条件下，分解是唯一的：</p>
<ol>
<li>至少 3 个不共线的光源方向</li>
<li>非Lambertian BRDF</li>
<li>足够的纹理变化</li>
</ol>
<p>形式化为条件数：
$$\kappa(\mathbf{J}) = \frac{\sigma_{\max}(\mathbf{J})}{\sigma_{\min}(\mathbf{J})} &lt; \tau$$
其中 $\mathbf{J}$ 是分解问题的 Jacobian 矩阵。</p>
<h2 id="143">14.3 生成模型作为先验</h2>
<h3 id="1431-vae">14.3.1 变分自编码器（VAE）先验</h3>
<p>将场景表示编码到潜在空间 $\mathbf{z} \in \mathbb{R}^d$：
$$q(\mathbf{z}|\Theta) = \mathcal{N}(\mu_\phi(\Theta), \Sigma_\phi(\Theta))$$
解码器生成场景参数：
$$p(\Theta|\mathbf{z}) = \mathcal{N}(\mu_\psi(\mathbf{z}), \Sigma_\psi(\mathbf{z}))$$
VAE 损失包含重建项和 KL 散度：
$$\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q(\mathbf{z}|\Theta)}[\log p(I_{\text{obs}}|\Theta)] - D_{\text{KL}}(q(\mathbf{z}|\Theta) | p(\mathbf{z}))$$
其中先验 $p(\mathbf{z}) = \mathcal{N}(0, \mathbf{I})$。</p>
<h3 id="1432">14.3.2 扩散模型在逆向渲染中的应用</h3>
<p>扩散模型通过逐步去噪过程生成数据：
$$\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_0 + \sqrt{1-\alpha_t} \boldsymbol{\epsilon}$$
在逆向渲染中，我们使用条件扩散模型：
$$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t, I_{\text{obs}}) = \mathcal{N}(\mu_\theta(\mathbf{x}_t, t, I_{\text{obs}}), \Sigma_\theta(t))$$
<strong>分数匹配目标</strong>：
$$\mathcal{L}_{\text{diffusion}} = \mathbb{E}_{t,\mathbf{x}_0,\boldsymbol{\epsilon}}\left[|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, I_{\text{obs}})|^2\right]$$</p>
<h3 id="1433">14.3.3 条件生成与后验采样</h3>
<p>给定观测 $I_{\text{obs}}$，我们需要采样后验分布：
$$p(\Theta|I_{\text{obs}}) \propto p(I_{\text{obs}}|\Theta) p(\Theta)$$
使用 <strong>Langevin 动力学</strong>：
$$\Theta_{k+1} = \Theta_k + \frac{\eta}{2} \nabla_\Theta \log p(\Theta_k|I_{\text{obs}}) + \sqrt{\eta} \boldsymbol{\xi}_k$$
其中 $\boldsymbol{\xi}_k \sim \mathcal{N}(0, \mathbf{I})$。</p>
<p>梯度可以分解为：
$$\nabla_\Theta \log p(\Theta|I_{\text{obs}}) = \nabla_\Theta \log p(I_{\text{obs}}|\Theta) + \nabla_\Theta \log p(\Theta)$$</p>
<h3 id="1434">14.3.4 先验强度与重建保真度的权衡</h3>
<p>总体目标函数：
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{recon}} + \lambda_{\text{prior}} \mathcal{L}_{\text{prior}} + \lambda_{\text{reg}} \mathcal{L}_{\text{reg}}$$
<strong>自适应权重策略</strong>：
$$\lambda_{\text{prior}}(k) = \lambda_0 \exp(-k/\tau)$$
随着优化进行，逐渐减少先验影响，允许更精确的重建。</p>
<p><strong>不确定性加权</strong>：
$$w(\mathbf{x}) = \frac{1}{\sigma^2_{\text{recon}}(\mathbf{x}) + \sigma^2_{\text{prior}}(\mathbf{x})}$$
在不确定区域更依赖先验。</p>
<h2 id="144">14.4 少样本重建</h2>
<h3 id="1441">14.4.1 元学习框架</h3>
<p>少样本重建的目标是从极少的观测（通常 1-5 张图像）恢复完整的三维场景。元学习提供了一个强大的框架：</p>
<p><strong>Model-Agnostic Meta-Learning (MAML)</strong> 应用于神经场：</p>
<p>初始化参数 $\Theta_0$ 通过多任务学习获得：
$$\Theta_0 = \arg\min_\Theta \sum_{i=1}^{N_{\text{tasks}}} \mathcal{L}_i(\Theta - \alpha \nabla_\Theta \mathcal{L}_i^{\text{support}}(\Theta))$$
其中 $\mathcal{L}_i^{\text{support}}$ 是任务 $i$ 在支持集上的损失。</p>
<p>对于新场景，快速适应：
$$\Theta_{\text{adapted}} = \Theta_0 - \alpha \sum_{k=1}^{K} \nabla_\Theta \mathcal{L}^{\text{query}}(\Theta)$$
<strong>条件神经过程 (CNP)</strong> 框架：</p>
<p>编码器聚合上下文信息：
$$\mathbf{r} = h\left(\frac{1}{N}\sum_{i=1}^N g(\mathbf{x}_i, I(\mathbf{x}_i))\right)$$
解码器基于全局表示预测：
$$p(I(\mathbf{x}_<em>)|I_{1:N}) = \mathcal{N}(\mu_\theta(\mathbf{x}_</em>, \mathbf{r}), \sigma^2_\theta(\mathbf{x}_*, \mathbf{r}))$$</p>
<h3 id="1442">14.4.2 神经表示的快速适应</h3>
<p><strong>超网络方法</strong>：使用超网络 $H$ 生成场景特定的权重：
$$\mathbf{W}_{\text{scene}} = H(\mathbf{z}_{\text{context}}; \Phi)$$
其中 $\mathbf{z}_{\text{context}}$ 是从观测图像提取的上下文向量。</p>
<p><strong>模块化适应</strong>：将网络分为共享模块和适应模块：
$$f(\mathbf{x}) = f_{\text{adapt}}(f_{\text{shared}}(\mathbf{x}; \Theta_{\text{shared}}); \Theta_{\text{adapt}})$$
只更新 $\Theta_{\text{adapt}}$，保持 $\Theta_{\text{shared}}$ 固定。</p>
<p><strong>低秩适应 (LoRA)</strong>：
$$\mathbf{W}_{\text{adapted}} = \mathbf{W}_0 + \Delta \mathbf{W} = \mathbf{W}_0 + \mathbf{B}\mathbf{A}$$
其中 $\mathbf{B} \in \mathbb{R}^{d \times r}$，$\mathbf{A} \in \mathbb{R}^{r \times k}$，$r \ll \min(d, k)$。</p>
<h3 id="1443">14.4.3 稀疏视图综合</h3>
<p><strong>几何先验</strong>：利用深度先验网络：
$$D_{\text{prior}}(\mathbf{x}) = f_{\text{depth}}(I_{\text{obs}}, \mathbf{x}; \Theta_{\text{depth}})$$
融合到密度预测：
$$\sigma(\mathbf{x}) = \sigma_{\text{neural}}(\mathbf{x}) + \lambda \cdot \delta(d(\mathbf{x}) - D_{\text{prior}}(\mathbf{x}))$$
<strong>多尺度特征聚合</strong>：
$$\mathbf{F}(\mathbf{x}) = \sum_{l=1}^L w_l \cdot \text{Interp}(\mathbf{F}_l, \mathbf{x})$$
其中 $\mathbf{F}_l$ 是第 $l$ 层的特征图。</p>
<p><strong>跨视图注意力机制</strong>：
$$\mathbf{f}_{\text{agg}}(\mathbf{x}) = \sum_{i=1}^N \text{softmax}\left(\frac{q(\mathbf{x}) \cdot k_i(\Pi_i(\mathbf{x}))}{\sqrt{d}}\right) v_i(\Pi_i(\mathbf{x}))$$
其中 $q$、$k$、$v$ 是查询、键、值变换。</p>
<h3 id="1444">14.4.4 不确定性量化</h3>
<p><strong>认知不确定性</strong>：通过集成方法估计：
$$\mu(\mathbf{x}) = \frac{1}{M}\sum_{m=1}^M f_m(\mathbf{x})$$</p>
<p>$$\sigma^2_{\text{epistemic}}(\mathbf{x}) = \frac{1}{M}\sum_{m=1}^M (f_m(\mathbf{x}) - \mu(\mathbf{x}))^2$$
<strong>偶然不确定性</strong>：直接预测：
$$f(\mathbf{x}) = (\mu(\mathbf{x}), \sigma^2_{\text{aleatoric}}(\mathbf{x}))$$
<strong>总不确定性</strong>：
$$\sigma^2_{\text{total}}(\mathbf{x}) = \sigma^2_{\text{epistemic}}(\mathbf{x}) + \sigma^2_{\text{aleatoric}}(\mathbf{x})$$
<strong>主动视图选择</strong>：基于信息增益选择下一个最佳视图：
$$\mathbf{v}_{\text{next}} = \arg\max_{\mathbf{v}} \mathbb{E}[H(\Theta) - H(\Theta|I_{\mathbf{v}})]$$
其中 $H$ 是熵函数。</p>
<h2 id="145">14.5 实时逆向渲染系统</h2>
<h3 id="1451">14.5.1 轻量级神经表示</h3>
<p><strong>哈希编码</strong>：使用多分辨率哈希表存储特征：
$$\mathbf{f}(\mathbf{x}) = \bigoplus_{l=1}^L \text{HashTable}_l[\text{hash}(\lfloor \mathbf{x} \cdot 2^l \rfloor)]$$
内存复杂度：$O(L \cdot T \cdot F)$，其中 $T$ 是表大小，$F$ 是特征维度。</p>
<p><strong>量化与剪枝</strong>：</p>
<ol>
<li><strong>权重量化</strong>：$w_q = \text{round}(w/s) \cdot s$</li>
<li><strong>结构化剪枝</strong>：移除重要性低的通道</li>
<li><strong>知识蒸馏</strong>：$\mathcal{L}_{\text{distill}} = |f_{\text{student}}(\mathbf{x}) - f_{\text{teacher}}(\mathbf{x})|^2$</li>
</ol>
<p><strong>张量分解</strong>：使用 CP 或 Tucker 分解：
$$\mathcal{T} \approx \sum_{r=1}^R \mathbf{a}_r \otimes \mathbf{b}_r \otimes \mathbf{c}_r$$</p>
<h3 id="1452">14.5.2 增量优化策略</h3>
<p><strong>关键帧策略</strong>：维护关键帧集合 $\mathcal{K}$：
$$\mathcal{K}_{t+1} = \begin{cases}
\mathcal{K}_t \cup {I_t} &amp; \text{if } d(I_t, \mathcal{K}_t) &gt; \tau \
\mathcal{K}_t &amp; \text{otherwise}
\end{cases}$$
<strong>局部更新</strong>：只更新观测区域的参数：
$$\Theta_{t+1}(\mathbf{x}) = \begin{cases}
\Theta_t(\mathbf{x}) - \eta \nabla \mathcal{L} &amp; \text{if } \mathbf{x} \in \mathcal{V}_{\text{observed}} \
\Theta_t(\mathbf{x}) &amp; \text{otherwise}
\end{cases}$$
<strong>滑动窗口优化</strong>：
$$\mathcal{L}_{\text{window}} = \sum_{i=t-W}^t w_{t-i} \mathcal{L}_i$$
其中 $w_i$ 是时间衰减权重。</p>
<h3 id="1453">14.5.3 硬件加速架构</h3>
<p><strong>GPU 并行化策略</strong>：</p>
<ol>
<li><strong>射线并行</strong>：每个 CUDA 线程处理一条射线</li>
<li><strong>体素并行</strong>：并行评估空间网格</li>
<li><strong>特征并行</strong>：在特征维度上分割计算</li>
</ol>
<p><strong>专用硬件设计</strong>：
$$\text{Throughput} = \frac{N_{\text{rays}} \times N_{\text{samples}}}{\text{Latency}_{\text{kernel}}}$$
优化目标：</p>
<ul>
<li>最大化内存带宽利用率</li>
<li>最小化分支发散</li>
<li>利用张量核心加速</li>
</ul>
<p><strong>混合精度训练</strong>：
$$\mathcal{L}_{\text{fp16}} = \text{scale} \times \mathcal{L}_{\text{original}}$$
使用动态损失缩放防止梯度下溢。</p>
<h3 id="1454">14.5.4 交互式编辑与反馈</h3>
<p><strong>实时预览</strong>：使用低分辨率代理：
$$I_{\text{preview}} = \text{Upsample}(\mathcal{R}_{\text{low}}(\Theta))$$
<strong>增量细化</strong>：
$$\Theta_{k+1} = \Theta_k + \alpha \cdot \text{UserEdit}(k)$$
<strong>约束传播</strong>：用户编辑作为硬约束：
$$\mathcal{L}_{\text{edit}} = \sum_{\mathbf{x} \in \mathcal{E}} |\Theta(\mathbf{x}) - \Theta_{\text{target}}(\mathbf{x})|^2$$
<strong>性能指标</strong>：</p>
<ul>
<li>延迟：$&lt; 16.7$ ms（60 FPS）</li>
<li>吞吐量：$&gt; 10^6$ 射线/秒</li>
<li>内存：$&lt; 4$ GB VRAM</li>
</ul>
<h2 id="_2">本章小结</h2>
<p>神经逆向渲染通过结合深度学习和物理渲染原理，实现了从图像到三维场景的逆向推断。关键贡献包括：</p>
<ol>
<li><strong>逆优化框架</strong>：将渲染方程的逆问题转化为可微分优化，通过梯度下降求解场景参数</li>
<li><strong>分解表示</strong>：通过物理约束和解耦损失，分离几何、材质和光照的内在成分</li>
<li><strong>生成先验</strong>：利用 VAE 和扩散模型编码场景统计，正则化不适定的逆问题</li>
<li><strong>少样本学习</strong>：通过元学习和条件神经过程，从极少观测恢复完整场景</li>
<li><strong>实时系统</strong>：通过轻量级表示和硬件优化，实现交互式逆向渲染</li>
</ol>
<p>核心公式回顾：</p>
<ul>
<li>逆向渲染目标：$\Theta^* = \arg\min_\Theta \mathcal{L}(\mathcal{R}(\Theta), I_{\text{obs}}) + \lambda \mathcal{R}_{\text{reg}}(\Theta)$</li>
<li>分解表示：$\mathbf{c}(\mathbf{x}, \boldsymbol{\omega}_o) = \mathbf{a}(\mathbf{x}) \odot \int_{\mathbb{S}^2} f_r \mathbf{l}(\mathbf{x}, \boldsymbol{\omega}_i) (\boldsymbol{\omega}_i \cdot \mathbf{n})^+ d\boldsymbol{\omega}_i$</li>
<li>后验采样：$p(\Theta|I_{\text{obs}}) \propto p(I_{\text{obs}}|\Theta) p(\Theta)$</li>
</ul>
<h2 id="_3">练习题</h2>
<h3 id="_4">基础题</h3>
<p><strong>练习 14.1</strong>：推导体积渲染中透射率 $T(t)$ 对网络参数 $\mathbf{W}$ 的梯度表达式。</p>
<p><em>提示</em>：使用链式法则和路径积分的导数。</p>
<details>
<summary>答案</summary>
<p>从透射率定义：
$$T(t) = \exp\left(-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds\right)$$
对 $\mathbf{W}$ 求导：
$$\frac{\partial T(t)}{\partial \mathbf{W}} = T(t) \cdot \frac{\partial}{\partial \mathbf{W}}\left[-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds\right]$$</p>
<p>$$= -T(t) \int_{t_n}^t \frac{\partial \sigma(\mathbf{r}(s))}{\partial \mathbf{W}} ds$$
这表明透射率梯度与路径上所有点的密度梯度相关。</p>
</details>
<p><strong>练习 14.2</strong>：证明在分解表示中，尺度歧义 $(k\mathbf{a}, \mathbf{l}/k)$ 保持渲染结果不变。</p>
<p><em>提示</em>：将缩放后的值代入渲染方程。</p>
<details>
<summary>答案</summary>
<p>原始渲染：
$$\mathbf{c} = \mathbf{a} \odot \mathbf{l}$$
缩放后：
$$\mathbf{c}' = (k\mathbf{a}) \odot (\mathbf{l}/k) = k \cdot \frac{1}{k} \cdot \mathbf{a} \odot \mathbf{l} = \mathbf{a} \odot \mathbf{l} = \mathbf{c}$$
因此渲染结果保持不变，这就是尺度歧义的来源。</p>
</details>
<p><strong>练习 14.3</strong>：计算哈希编码的内存需求，给定 $L=16$ 层，每层表大小 $T=2^{19}$，特征维度 $F=2$。</p>
<p><em>提示</em>：总内存 = 层数 × 表大小 × 特征维度 × 每个浮点数字节。</p>
<details>
<summary>答案</summary>
<p>内存需求：
$$\text{Memory} = L \times T \times F \times 4 \text{ bytes}$$
$$= 16 \times 2^{19} \times 2 \times 4$$
$$= 16 \times 524288 \times 8$$
$$= 67,108,864 \text{ bytes} \approx 64 \text{ MB}$$
这是相对紧凑的表示，适合实时应用。</p>
</details>
<h3 id="_5">挑战题</h3>
<p><strong>练习 14.4</strong>：设计一个联合优化框架，同时进行场景重建和相机标定。写出目标函数和优化策略。</p>
<p><em>提示</em>：考虑相机参数 $\mathbf{P}$ 和场景参数 $\Theta$ 的联合优化。</p>
<details>
<summary>答案</summary>
<p>联合优化目标：
$$(\Theta^<em>, \mathbf{P}^</em>) = \arg\min_{\Theta, \mathbf{P}} \sum_{i=1}^N \mathcal{L}_{\text{photo}}(\mathcal{R}(\Theta, \mathbf{P}_i), I_i) + \lambda_1 \mathcal{R}_{\text{scene}}(\Theta) + \lambda_2 \mathcal{R}_{\text{camera}}(\mathbf{P})$$
其中相机正则化：
$$\mathcal{R}_{\text{camera}}(\mathbf{P}) = \sum_{i,j} |\mathbf{P}_i - \mathbf{P}_j|^2_{\text{smooth}} + \sum_i |\mathbf{P}_i - \mathbf{P}_i^{\text{init}}|^2$$
交替优化策略：</p>
<ol>
<li>固定 $\mathbf{P}$，优化 $\Theta$：$\Theta_{k+1} = \Theta_k - \eta_\Theta \nabla_\Theta \mathcal{L}$</li>
<li>固定 $\Theta$，优化 $\mathbf{P}$：$\mathbf{P}_{k+1} = \mathbf{P}_k - \eta_P \nabla_P \mathcal{L}$</li>
<li>重复直到收敛</li>
</ol>
<p>关键是平衡两者的学习率，防止退化解。</p>
</details>
<p><strong>练习 14.5</strong>：推导条件扩散模型在逆向渲染中的分数函数 $\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t|I_{\text{obs}})$。</p>
<p><em>提示</em>：使用贝叶斯定理和分数匹配。</p>
<details>
<summary>答案</summary>
<p>从贝叶斯定理：
$$p(\mathbf{x}_t|I_{\text{obs}}) = \frac{p(I_{\text{obs}}|\mathbf{x}_t) p(\mathbf{x}_t)}{p(I_{\text{obs}})}$$
取对数梯度：
$$\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t|I_{\text{obs}}) = \nabla_{\mathbf{x}_t} \log p(I_{\text{obs}}|\mathbf{x}_t) + \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t)$$
第二项是无条件分数，由预训练模型提供：
$$\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t) = -\frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1-\alpha_t}}$$
第一项需要通过渲染计算：
$$\nabla_{\mathbf{x}_t} \log p(I_{\text{obs}}|\mathbf{x}_t) = -\frac{1}{\sigma^2} \nabla_{\mathbf{x}_t} |\mathcal{R}(\mathbf{x}_t) - I_{\text{obs}}|^2$$
这需要可微渲染器 $\mathcal{R}$。</p>
</details>
<p><strong>练习 14.6</strong>：分析元学习 MAML 在神经场中的计算复杂度，包括前向传播和反向传播。</p>
<p><em>提示</em>：考虑内外循环的梯度计算。</p>
<details>
<summary>答案</summary>
<p>设网络有 $P$ 个参数，批大小为 $B$，内循环步数为 $K$。</p>
<p><strong>内循环（适应）</strong>：</p>
<ul>
<li>前向：$O(KBP)$</li>
<li>反向：$O(KBP)$</li>
</ul>
<p><strong>外循环（元优化）</strong>：</p>
<ul>
<li>需要计算二阶导数</li>
<li>前向：$O(BP)$</li>
<li>反向：$O(KBP^2)$（由于需要通过内循环梯度）</li>
</ul>
<p><strong>总复杂度</strong>：$O(N_{\text{tasks}} \cdot K \cdot B \cdot P^2)$</p>
<p>内存需求：需要存储计算图，$O(KP)$</p>
<p>优化：使用一阶近似（FOMAML）降低到 $O(KBP)$。</p>
</details>
<p><strong>练习 14.7</strong>（开放题）：设计一个结合物理仿真的神经逆向渲染系统，用于推断动态场景的物理参数（如质量、弹性系数）。</p>
<p><em>提示</em>：考虑如何将物理约束集成到优化框架中。</p>
<details>
<summary>答案</summary>
<p>系统设计：</p>
<ol>
<li>
<p><strong>表示</strong>：
   - 静态几何：$\Theta_{\text{geo}}$
   - 物理参数：$\Theta_{\text{phys}} = {m_i, k_i, c_i}$（质量、刚度、阻尼）
   - 动态状态：$\mathbf{x}_i(t), \mathbf{v}_i(t)$</p>
</li>
<li>
<p><strong>物理约束</strong>：
$$\mathbf{F} = m\mathbf{a} = -k\mathbf{x} - c\mathbf{v}$$</p>
</li>
<li>
<p><strong>优化目标</strong>：
$$\mathcal{L} = \mathcal{L}_{\text{render}} + \lambda_1 \mathcal{L}_{\text{physics}} + \lambda_2 \mathcal{L}_{\text{temporal}}$$
其中：</p>
</li>
</ol>
<ul>
<li>$\mathcal{L}_{\text{physics}} = \sum_t |m\ddot{\mathbf{x}} + k\mathbf{x} + c\dot{\mathbf{x}}|^2$</li>
<li>$\mathcal{L}_{\text{temporal}} = \sum_t |\mathbf{x}_{t+1} - \text{Integrate}(\mathbf{x}_t, \mathbf{v}_t, \Theta_{\text{phys}})|^2$</li>
</ul>
<ol start="4">
<li>
<p><strong>可微分仿真</strong>：使用隐式积分器保证稳定性</p>
</li>
<li>
<p><strong>应用</strong>：材质识别、碰撞预测、机器人抓取规划</p>
</li>
</ol>
</details>
<p><strong>练习 14.8</strong>：证明在少样本设置下，使用 $N$ 个视图时重建误差的期望界限。</p>
<p><em>提示</em>：使用 PAC 学习理论。</p>
<details>
<summary>答案</summary>
<p>设场景空间复杂度为 $\mathcal{H}$，使用 $N$ 个独立同分布的视图。</p>
<p>根据 PAC 界限，以概率至少 $1-\delta$：
$$\mathbb{E}[\mathcal{L}_{\text{test}}] \leq \mathcal{L}_{\text{train}} + \sqrt{\frac{2\log(|\mathcal{H}|/\delta)}{N}} + \mathcal{B}_{\text{approx}}$$
其中：</p>
<ul>
<li>$\mathcal{L}_{\text{train}}$：训练误差</li>
<li>$|\mathcal{H}|$：假设空间大小（与网络容量相关）</li>
<li>$\mathcal{B}_{\text{approx}}$：函数逼近误差</li>
</ul>
<p>对于神经场，$\log|\mathcal{H}| \approx O(P \log P)$，其中 $P$ 是参数数量。</p>
<p>因此：
$$\mathbb{E}[\mathcal{L}_{\text{test}}] = O\left(\mathcal{L}_{\text{train}} + \sqrt{\frac{P \log P}{N}}\right)$$</p>
<p>这表明需要 $N = \Omega(P \log P)$ 个视图才能获得良好的泛化。</p>
</details>
<h2 id="_6">常见陷阱与错误</h2>
<ol>
<li>
<p><strong>梯度消失/爆炸</strong>
   - 问题：深层网络中梯度不稳定
   - 解决：使用梯度裁剪、层归一化、残差连接</p>
</li>
<li>
<p><strong>局部极小值</strong>
   - 问题：非凸优化陷入次优解
   - 解决：多次随机初始化、课程学习、模拟退火</p>
</li>
<li>
<p><strong>分解歧义</strong>
   - 问题：材质-光照分离不唯一
   - 解决：强物理先验、多视图约束、参考标定</p>
</li>
<li>
<p><strong>过拟合</strong>
   - 问题：少样本设置下记忆训练视图
   - 解决：正则化、数据增强、早停</p>
</li>
<li>
<p><strong>计算效率</strong>
   - 问题：实时要求与精度冲突
   - 解决：自适应采样、层级表示、混合精度</p>
</li>
<li>
<p><strong>数值稳定性</strong>
   - 问题：体积渲染积分的数值误差
   - 解决：对数空间计算、稳定的积分方案</p>
</li>
</ol>
<h2 id="_7">最佳实践检查清单</h2>
<h3 id="_8">设计阶段</h3>
<ul>
<li>[ ] 明确逆向渲染目标（几何/材质/光照）</li>
<li>[ ] 选择合适的神经表示（隐式/显式/混合）</li>
<li>[ ] 设计物理合理的分解架构</li>
<li>[ ] 确定先验知识的引入方式</li>
<li>[ ] 规划计算资源和实时性要求</li>
</ul>
<h3 id="_9">实现阶段</h3>
<ul>
<li>[ ] 实现稳定的梯度计算和反向传播</li>
<li>[ ] 添加必要的正则化项</li>
<li>[ ] 设置合理的初始化策略</li>
<li>[ ] 实现高效的采样和积分方案</li>
<li>[ ] 优化内存使用和计算并行性</li>
</ul>
<h3 id="_10">评估阶段</h3>
<ul>
<li>[ ] 定量评估重建精度（PSNR, SSIM, LPIPS）</li>
<li>[ ] 测试泛化能力（新视角、新光照）</li>
<li>[ ] 分析计算性能（FPS, 内存占用）</li>
<li>[ ] 验证物理合理性（能量守恒、互易性）</li>
<li>[ ] 用户研究（如适用）</li>
</ul>
<h3 id="_11">部署阶段</h3>
<ul>
<li>[ ] 模型压缩和量化</li>
<li>[ ] 硬件适配优化</li>
<li>[ ] 鲁棒性测试（噪声、遮挡）</li>
<li>[ ] 增量更新机制</li>
<li>[ ] 用户交互接口</li>
</ul>
<hr />
<p><em>下一章：<a href="chapter15.html">第15章：标量波动光学基础</a></em></p>
            </article>
            
            <nav class="page-nav"><a href="./chapter13.html" class="nav-link prev">← 第13章：材质与几何重建</a><a href="./chapter15.html" class="nav-link next">第15章：标量波动光学基础 →</a></nav>
        </main>
    </div>
</body>
</html>